import nltk
from nltk.stem.wordnet import WordNetLemmatizer as wnl
from nltk.corpus import wordnet
import itertools
import re
import pandas as pd
import numpy as np
import copy
import random
import csv
import xlrd
from thesaurus import Word
import spacy
import pprint

"""
Some Queries the students had created had some words removed, instead of being replaced
by a *. For such queries, we created functions that can remove words. 

Remove Nouns from the sentence
Remove Adjectives from the sentence
Remove Quantifiers from the sentence
Remove Adverbs from the sentence

All these functions return a list of lists. 
Each list had the [word, pos-tag]. 
['Apple', 'NNP']
"""

apo_list = ["n't", "'ve", "'s", "'ll", "'re", "'d"]
special_chars = [',', '.', '!', '?', ';']
list_of_nouns = ['NN', 'NNP', 'NNPS', 'NNS']
list_of_pronouns = ['PRP', 'PRP$']
list_of_ads = ['PDT', 'RBR', 'RBS', 'DT', 'TO']


def remove_nouns(tagged_text):
    res = []
    for i in tagged_text:
        if i[0] in special_chars:
            pass
        elif i[1] in list_of_nouns:
            res.append([' ', i[1]])
        else:
            res.append([i[0], i[1]])
    return res


def remove_adjs(tagged_text):
    res = []
    for i in tagged_text:
        if i[1] in list_of_ads or i[0] in special_chars:
            pass
        else:
            res.append([i[0], i[1]])
    return res


def remove_jjs(tagged_text):
    tagged = tagged_text
    temp_res = []
    for i in tagged_text:
        if i[1] == 'JJ':
            temp_res.append(i[0])
    result_set = set()
    temp_tagged = tagged
    for i in range(len(temp_res)+1):
        for j in itertools.combinations_with_replacement(temp_res, i):
            for k in j:
                temp_tagged = [[' ', x[1]] if x[0] == k else [x[0], x[1]] for x in tagged]
            result_set.add(convert_to_text(temp_tagged))
            temp_tagged = tagged
    return result_set


def remove_quants(tagged_text):
    res = []
    for i, j in enumerate(tagged_text):
        try:
            if j[1] == "RB" and i + 1 < len(tagged_text) and tagged_text[i + 1][1] == "RB":
                continue
            elif j[1] == "RB" and i + 1 < len(tagged_text) and tagged_text[i + 1][1] == "JJ":
                continue
            else:
                res.append([j[0], j[1]])
        except:
            print (i, j)
    return res


# Calculate the depth of a nested list

depth = lambda L: isinstance(L, list) and max(map(depth, L))+1

"""
Use Spacy to POS tag a sentence a return a list of lists. 
Sentence: Apple is a fruit. 
Output: [['Apple', 'NNP'], ['is', 'VBZ'], ['a', 'DT'], ['fruit', 'NN']]
"""
def return_tagged_sentence(sentence):
    # text = nltk.word_tokenize(sentence)
    nlp = spacy.load('en_core_web_sm')
    doc = nlp(unicode(sentence))
    tagged_text = []
    for token in doc:
        tagged_text.append([token.text, token.tag_])

    return tagged_text

# Read the Excel sheet with all the data using Pandas
def return_sentence():
    df = pd.read_excel('all_sentences.xlsx')
    return np.asarray(df['wsc_problem']), np.asarray(df['search_query'])

# Convert the list of lists to a sentence. Append only the words and ignore the POS tags.
def convert_to_text(tagged_text):
    res = ''
    if tagged_text:
        for i in tagged_text:
            if res and res.strip(" ").split(" ")[-1] == "*" and i[0].strip(" ") == "*":
                pass
            else:
                res = res + ' ' + i[0]
        for i in apo_list:
            pos = res.find(i)
            if pos != -1:
                if res[pos - 1] == ' ':
                    res = res[:pos - 1] + res[pos:]
    return res.strip(" ")

"""
# Combinatorial replacement. 
def replace_pronouns(tagged_text):
    temp_res = []
    for j, i in enumerate(tagged_text):
        if i[1] in list_of_pronouns:
            temp_res.append(j)
    result_set = []
    for i in range(len(temp_res) + 1):
        temp_tagged = tagged_text
        for j in itertools.combinations_with_replacement(temp_res, i):
            for k in j:
                temp_tagged = [['*', x[1]] if nc == k else [x[0], x[1]] for nc, x in enumerate(temp_tagged)]
                result_set.append(temp_tagged)
    return result_set
"""

"""
Replace Pronouns, Quantifiers, Nouns, Adjectives, Adverbs
Based on the POS-tags generated by Spacy. 
"""
def replace_pronouns(tagged_text):
    res = []
    for i in tagged_text:
        if i[0] in special_chars:
            pass
        elif i[1] in list_of_pronouns:
            res.append(['*', i[1]])
        else:
            res.append([i[0], i[1]])
    return res

def replace_nouns(tagged_text):
    res = []
    for i in tagged_text:
        if i[0] in special_chars:
            pass
        elif i[1] in list_of_nouns:
            res.append(['*', i[1]])
        else:
            res.append([i[0], i[1]])
    return res


def replace_adjs(tagged_text):
    res = []
    for i in tagged_text:
        if i[1] in list_of_ads or i[0] in special_chars:
            res.append(['*', i[1]])
        else:
            res.append([i[0], i[1]])
    return res


def replace_quants(tagged_text):
    res = []
    for i, j in enumerate(tagged_text):
        try:
            if j[1] == "RB" and i + 1 < len(tagged_text) and tagged_text[i + 1][1] == "RB":
                res.append(['*', j[1]])

            elif j[1] == "RB" and i + 1 < len(tagged_text) and tagged_text[i + 1][1] == "JJ":
                res.append(['*', j[1]])
            else:
                res.append([j[0], j[1]])
        except:
            print (i, j)
    return res


def replace_jjs(tagged_text):
    temp_res = []
    for i in tagged_text:
        if i[1] == 'JJ' or i[1] == 'IN':
            temp_res.append(i[0])
    result_set = []
    for i in range(len(temp_res)+1):
        temp_tagged = tagged_text
        for j in itertools.combinations_with_replacement(temp_res, i):
            for k in j:
                temp_tagged = [['*', x[1]] if x[0] == k else [x[0], x[1]] for x in temp_tagged]
                result_set.append(temp_tagged)
    return result_set

if __name__ == "__main__":
    # #
    # sentence = "Billy cried because Toby wouldn't accept * toy and his boy."
    # tagged_sentence = return_tagged_sentence(sentence)
    # temp = eval('replace_pronouns'+'(tagged_sentence)')
    # pprint.pprint(temp)


    # Get sentences from the Excel Sheet

    wsc_problems, search_queries = return_sentence()
    print("Total sentences ===>" + str(len(wsc_problems)))
    start_count = 0
    # Perform combinations of functions on the sentence
    # and for each combination, generate a query and append the output file.
    functions = ["replace_adjs", "replace_quants", "replace_jjs"]
    result_functions = []
    for length in range(1, len(functions) + 1):
        result_functions.extend(list(itertools.combinations(functions, length)))

    for problem, query in zip(wsc_problems, search_queries):
        print("Processing " + str(start_count + 1) + " line!")
        with open('spacy_queries.txt', 'a') as the_file:
            the_file.write('\n\n' + str(str(problem).encode('utf-8')) + '\n')
            if type(query) is float:
                the_file.write('\n\n' + str(str(query).encode('utf-8')) + '\n\n')
            else:
                the_file.write('\n\n' + str(query.encode('utf-8')) + '\n\n')
        # Split the WSC sentence and remove the question.
        # The man lifted the boy onto his shoulders. Whose shoulders? Answer: the man's.
        # Only - The man lifted the boy onto his shoulders. - Should be considered
        final_sentence_list = set()
        a = problem.rsplit('?', 1)[0]
        sentence = a.rsplit('.', 1)[0]

        tagged_sent = return_tagged_sentence(sentence)
        VBS = [word for word, pos in tagged_sent if pos in ['VB', 'VBD', 'VBN', 'VBG', 'VBP', 'VBZ']]

        sentence_set = list()
        t_sentence = return_tagged_sentence(sentence)
        sentence_set.append(t_sentence)
        kk = sentence.split()
        fin_kk = []
        # For each verb use a different tense and create a sentence
        # Add that to the sentence set
        for i, word in enumerate(kk):
            if word in VBS:
                fin_kk.append(wnl().lemmatize(word, 'v'))
            else:
                fin_kk.append(word)
        sentence_set.append(return_tagged_sentence(" ".join(fin_kk)))

        nouns_replaced = replace_nouns(t_sentence)
        sentence_set.append(nouns_replaced)
        final_sentence_list.add(convert_to_text(nouns_replaced))

        sentence_set.append(replace_pronouns(nouns_replaced))
        final_sentence_list.add(convert_to_text(replace_pronouns(nouns_replaced)))

        # for each_pro in replace_pronouns(nouns_replaced):
        #     final_sentence_list.add(convert_to_text(each_pro))
        #     sentence_set.append(each_pro)

        # Sentence set had the original sentence and the sentence with the different
        # tenses for each verb. Do all the functions for each of these sentences.
        for sentence in sentence_set:
            for funcs in result_functions:
                temp = sentence
                for func in funcs:
                    temp = eval(str(func) + '(temp)')
                    if type(temp) is list and len(temp) != 0 and depth(temp) == 2:
                        final_sentence_list.add(convert_to_text(temp))
                    elif type(temp) is list and len(temp) != 0 and depth(temp) == 3:
                        for sent_temp in temp:
                            final_sentence_list.add(convert_to_text(sent_temp))

        # Show progress and write to the output File.
        print("Writing " + str(start_count + 1) + " string to file!")
        start_count += 1
        for sent in final_sentence_list:
            write_sentence = str(sent).replace("'s", "")
            write_sentence = re.sub("\s\s+", " ", write_sentence)
            write_sentence = re.sub("\* \*", "*", write_sentence)
            with open('spacy_queries.txt', 'a') as the_file:
                the_file.write(str(write_sentence) + '\n')
